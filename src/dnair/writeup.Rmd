---
title: "Parsing SEC Filings to Identify Product Innovation: Working Document"
output:
  html_document:
    self_contained: no
---

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>

```{r libraries_1, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, results = "hide"}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, results = "asis")
library(readr)
library(dplyr)
library(stringr)
library(xml2)
library(rvest)
library(stringr)
library(hunspell)
library(data.table)
library(htmltools)
library(magrittr)
library(htmltidy)
library(ggplot2)

library(wesanderson)
#install.packages("rworldmap")
library(rworldmap)
```

```{r functions_1}

remove_doc_types <- function(xml_string, types = c("GRAPHIC", "EXCEL", "ZIP", "EX-10.3", "EX-10.6", "EX-10.20")) {
  no_ns <- gsub("\\n", " ", xml_string)
  #browser()
  for (t in types) {
    find_str <- paste0("<DOCUMENT> ?<TYPE> ?", t)
    search_str <- paste0("<DOCUMENT> ?<TYPE> ?", t, ".*?</DOCUMENT>")
    found <-
      as.data.table(stringr::str_locate_all(no_ns, find_str))

    for (i in 1:nrow(found)) {
      locs <- as.data.table(stringr::str_locate(no_ns, search_str))
      st <- locs[1, start] - 1
      en <- locs[1, end] + 1
      ifelse(is.na(locs$start) == TRUE & is.na(locs$end) == TRUE, no_ns,
             no_ns <- paste0(substr(no_ns, 1, st), substr(no_ns, en, nchar(no_ns))) )
    }
  }
  no_ns
}

```

```{r load_1, results = "hide"}
paths_file <- "~/git/business_innovation/data/original/edgar_filings/ALL_SEC_files.txt"
file_headers <- readr::read_tsv(paths_file, col_names = FALSE)
paths <- paste0("~/git/business_innovation/data/original/edgar_filings/Edgar_filings_folders/", file_headers$X1)
file_names <- unique(list.files(paths, full.names = TRUE))

# wordcounts_1_1000 <- read_csv("~/git/business_innovation/data/working/sec/wordcounts_1_1000.csv")
# wordcounts_1001_2000 <- read_csv("~/git/business_innovation/data/working/sec/wordcounts_1001_2000.csv")
# wordcounts_2000_2867 <- read_csv("~/git/business_innovation/data/working/sec/wordcounts_2000_2867.csv")
# allwordcounts <- as.data.table(rbind(wordcounts_1_1000, wordcounts_2000_2867, wordcounts_2000_2867))
#saveRDS(allwordcounts, "~/git/business_innovation/data/working/sec/all_wordlist.RDS")
allwordcounts <- readRDS("~/git/business_innovation/data/working/sec/all_wordlist.RDS")

ciknames <- read_rds("~/git/business_innovation/data/original/ciks_names.RDS")
sic <- read_rds("~/git/business_innovation/data/original/sic.download.RDS")
cik_ticker <- read_delim("~/git/business_innovation/data/original/edgar_filings/cik_ticker.csv", delim = "|")   #RANKANDFILE website
cikcountries <- read_delim("~/git/business_innovation/data/original/edgar_filings/edgar_state_country.csv", delim = "|") #RANKANDFILE website

allwords <- read_csv("~/git/business_innovation/data/working/sec/all_secwordlists.csv") # 4M TOTAL WORDS ACROSS ALL FILINGS
allregwords <- read_csv("~/git/business_innovation/data/working/sec/all_secregwordlists.csv") # 37K REG WORDS ACROSS ALL FILINGS
#allwordcounts = 15K NON ENGLISH WORDS ACROSS ALL FILINGS

colnames(sic) <- c("CIK", "SIC_CompName", "SIC_SIC", "SIC_Industry", "SIC_Location")
colnames(ciknames) <- c("CIK", "SEC_CompName", "SEC_SIC")
colnames(cik_ticker) <- c("CIK", "Ticker_TickerCode", "Ticker_CompName", "Ticker_Exchange", "Ticker_SIC", "Ticker_Location", "Ticker_Inc_Location", "Ticker_IRS")
colnames(cikcountries) <- c("Code", "Ticker_StateCountry")
```

```{r joinprep_1}
sic$SIC_SIC <- as.numeric(sic$SIC_SIC)
sic$CIK <- as.numeric(sic$CIK)
ciknames$CIK <- as.numeric(ciknames$CIK)
ciknames$SEC_SIC <- as.numeric(ciknames$SEC_SIC)
#incorp <- cik_ticker %>% select(CIK, Incorporated)
```

```{r reshape_1}
wcbycomp <- reshape2::dcast(allwordcounts, Company + Words ~ Year, value.var = "count", fun.aggregate = sum)
wcbyword <- reshape2::dcast(allwordcounts, Words + Company ~ Year, value.var = "count", fun.aggregate = sum)
wcbycompdetails <- wcbycomp %>%
  left_join(ciknames, by = c("Company" = "CIK")) %>%
  left_join(sic, by = c("Company" = "CIK", "SEC_SIC" = "SIC_SIC")) %>%
  left_join(cik_ticker, by = c("Company" = "CIK"))
```

```{r comp_reference_1}
#length(file_names) # 2867 filings
#length(unique(ciknames$CIK))  # 779 - total # of companies classified as pharm/med device by SEC 

patt1 <- "(?<=Edgar_filings_folders/)(.*)(?=.txt)"
patt2 <- "(?<=/)(.*)(?=_10-K_)"
orig_companies <- str_extract(str_extract((file_names), patt1), patt2)

#length(unique(orig_companies$orig_companies)) #703 companies with filings
#length(unique(wcbycomp$Company)) # 365 - # of companies that we found non-English words in their filings

orig_companies <- as.numeric(orig_companies)
orig_companies <- as.data.frame(orig_companies)

origcomp_details <- orig_companies %>%
  filter(!is.na(orig_companies)) %>% 
  unique() %>% 
  left_join(ciknames, by = c("orig_companies" = "CIK")) %>%
  left_join(sic, by = c("orig_companies" = "CIK", "SEC_SIC" = "SIC_SIC")) %>%
  left_join(cik_ticker, by = c("orig_companies" = "CIK"))

#saveRDS(origcomp_details, "~/git/business_innovation/data/business_innovation/working/sec/origcomp_details.RDS")

cikcountries <- cikcountries %>%
  mutate(US = recode(Code, 
                     "CA"= "USA", "CO" = "USA", "CT" = "USA", "DC" = "USA", "FL" = "USA", "GA" = "USA",
                     "IL" = "USA", "IN" = "USA", "MA" = "USA", "MD" = "USA", "MN" = "USA", "MI" = "USA",
                     "MO" = "USA", "NC" = "USA", "NJ" = "USA", "NV" = "USA", "NY" = "USA", "OH" = "USA",
                     "PA" = "USA", "SC" = "USA", "TN" = "USA", "TX" = "USA", "UT" = "USA", "WA" = "USA",
                     "AK" = "USA", "AL" = "USA", "AR" = "USA", "AZ" = "USA", "DE" = "USA", "IA" = "USA",
                     "ID" = "USA", "ME" = "USA", "MS" = "USA", "MT" = "USA", "ND" = "USA", "NE" = "USA",
                     "OK" = "USA", "OR" = "USA", "PR" = "USA", "RI" = "USA", "SD" = "USA", "VA" = "USA",
                     "VT" = "USA", "WA" = "USA", "WI" = "USA", "WV" = "USA", "WY" = "USA", "NH" = "USA")) 

```




```{r libraries2}
library(corpus)
```

```{r functions2}
stem_hunspell <- function(term) {
  # look up the term in the dictionary
  stems <- hunspell::hunspell_stem(term)[[1]]

  if (length(stems) == 0) { # if there are no stems, use the original term
    stem <- term
  } else { # if there are multiple stems, use the last one
    stem <- stems[[length(stems)]]
  }

  stem
}

'%!in%' <- function(x,y)!('%in%'(x,y))
```

```{r load_2}
regwords <- read_csv("~/git/business_innovation/data/working/sec/all_secregwordlists.csv")
companylist <- readRDS("~/git/business_innovation/data/working/sec/companylist.RDS")
```


```{r remind_me_3_sets_are_1_and_2, eval=FALSE}
# 3 word datasets
allwords # 4 million words across all filings
allwordcounts # 15K non-English words across all filings 
allregwords # 37K protected words across all filings
```


### Introduction

This project aims to test the feasibility to identify, measure, and characterize product innovation using non-survey data sources. Our goal is to develop methods to complement and enhance the BRDIS survey that collects information from a representative set of companies and asks whether they have introduced a new product to the market. Specifically, we are developing text-based methods to be applied to administrative (e.g., financial filings) and opportunity data (e.g., trade journals, press releases) to determine:

* whether a company has launched a new product?
* how many new products are introduced?
* what are the features of the new product(s)?
* how that innovation trends over time?

To answer these questions, we measure innovation in terms of products, and seek to capture a new product and characterize its trajectory across a number of text based sources. Additionally, we focus on the pharmaceutical and medical device industry that are highly regulated such that new products require approval from the Food and Drug Administration (FDA). We make the assumption that the FDA approval dataset (which is publicly available) is the universal set of all new products, and ask what the portion of these products we can capture using administrative and opportunity data. Their respective SIC codes are given here. 

```{r illustrate_industries_1}
knitr::kable(tibble("SIC" = unique(origcomp_details$SEC_SIC),
       "Industry" = unique(origcomp_details$SIC_Industry)) %>% filter(!is.na(SIC)))
```

We chose to focus on the pharmaceutical and medical device industry because of the strictly regulated process of launching new products that is specific to this industry. Consider the following as a high-level illustration of the process a new drug or medical device might take to the market:

1. Research & Development: Company undertakes research to develop, test, and trial new device and drug. 
2. FDA Application: Company submits application to FDA for device or drug approval.
3. Approval Announcements: FDA releases announcements to the public. 
4. Press Activity: Media outlets report on the announcements, company and competitor relationships, and launches to market.
5. Market Activity: Company retails device or drug.
6. Financial Reports: Company submits financial reports to US Securities & Exchange Commission (SEC). 

```{r illustrate_counts_1}
# class(origcomp_details)
# origcomp_details$Industry <- subset.data.frame(x = origcomp_details, select = 8)
compcount1 <- origcomp_details %>% 
    group_by(SIC_Industry) %>%
    summarise(no_companies = n())

compcount2 <- ciknames %>% 
  left_join(sic, by = c("CIK", "SEC_SIC" = "SIC_SIC")) %>% 
  group_by(SIC_Industry) %>%
  summarise(no_companies = n())

compcount3 <- allwordcounts %>% 
  group_by(Company, Year) %>% 
  summarise(count = n())
```

```{r illustrate_counts_2}
compcount4 <- allregwords %>% 
  group_by(Company, Year) %>% 
  summarise(count = n())
```


By tracing products through their lifecycle from FDA approval through financial impact, we can expand upon the BRDIS survey results by providing additional context and information around what innovation looks like in the pharmaceutical industry. Furthermore, we can illustrate a process by which innovation can be uncovered and measured, at least in a highly regulated environment. 

### Data Source: SEC Filings

This portion of the research focused on the last stage of the lifecycle: whether a new product appears in the financial activities of a company. For this we used the SEC's EDGAR database of 10-K filings. Using the criteria of the two industries of interest, we identified `r length(unique(ciknames$cik))` companies in their database as belonging to the pharmaceutical/medical device industries. Of these, `r nrow(unique(orig_companies))` filed 10-K forms with the SEC, which report on their financial well-being as a company. 


Table below summarizes the number of companies on Edgar.

Company Sets | N
------------- | -------------
Total Number of Pharma Companies on Edgar | `r length(unique(ciknames$CIK))`
SIC 2834: Drugs| `r as.integer(compcount2["1",2]) `
SIC 3841: Devices | `r as.integer(compcount2["2",2]) `
Number Pharma Companies w 10-K| `r nrow(unique(orig_companies))`
SIC 2834: Drugs| `r as.integer(compcount1["2",2]) `
SIC 3841: Devices | `r as.integer(compcount1["3",2]) `

We collected the 10-K filings of the `r nrow(unique(orig_companies))` companies for the years 2013--2016, which makes a total of `r length(file_names)` filings. 

Filings Sets | N
------------- | -------------
Total 10-K Filings  | `r length(file_names)`
10-K Filings w non-English | `r nrow(compcount3) `
10-K Filings w Protected Brand | `r nrow(compcount4) `

### Methods: Natural Language Processing 

**Outline for myself:**

1. Ingest text
2. Length check
3. Candidate product capture 
    a) Paragraphs containing launch/new product --> non-English words
    b) ANY paragraph --> R/TM symbol
4. Refine capture results
    a) Company name dictionary
    b) Pharma term dictionary


Our goal is to identify mentions of product launches for a given year using the 10-K filings of pharmaceutical companies. This is a challenging task, given the hundreds of filings in our dataset and the thousands of words within each filing. We specifically needed to find a way to: 

1. limit the body of text down to sections of the filings that describe products 
2. identify specific words most likely to represent a product

Below we step through our iterative process using one of the filings as an example (#1 above), and then walk through our two approaches for product capture (#2)

##### Step 1: Ingest the text of an SEC filing

First the text of all filings was ingested, such that each text element represented an observation of the dataset. An example SEC filing can be found here: https://www.sec.gov/Archives/edgar/data/310158/000119312512084319/d274705d10k.htm.

Here's the text using the above linked Merck filing as an example. 

```{r illustrate_sectext_1}
url <- "https://www.sec.gov/Archives/edgar/data/310158/000119312512084319/d274705d10k.htm"
edgar <- read_html(remove_doc_types(read_file(url)))
paragraphs <- edgar %>% 
  xml_find_all( ".//p") %>% 
  html_text() %>%
  str_squish

knitr::knit_print(paragraphs[dataplumbr::var.is_blank(paragraphs) == FALSE][4:10])
```

##### Step 2: Filter text elements of significant length

There were `r length(paragraphs)` text elements overall in this filing. For each of these text elements, we selected only those elements that were at least 20 characters. This reduces the example by `r length(paragraphs) - length(paragraphs[nchar(paragraphs)>20])` to `r length(paragraphs[nchar(paragraphs)>20])`. 

```{r illustrate_sectextbylength_1}
knitr::knit_print(paragraphs[nchar(paragraphs)>20][4:10])
```


##### Step 3: Candidate Product Capture

Given the 10-K filing of a company, our goal is to identify mentions of product launches for a given year. This requires identifying product names in these filings, and finding those that are mentioned as being launched in the respective year. Currently, we are developing two parallel approaches which will be combined eventually. The first one involves obtaining non-English words in a filing and identifying those that are used in close "proximity" with our keywords of innovation, i.e., launch, new product (list to be expanded). Among the `r length(file_names)` filings, `r nrow(compcount3) ` referenced non-English words in their filings. The second approach is to identify names that are used with a trademark or a registered trademark sign.

We wanted to determine the number and names of the new products and took two approaches to capturing candidate product words in the SEC 10-K filing text. 

**A) Searching text for non-English words**

We then wanted to identify text elements that contained our target innovation phrases, so we looked for elements that contained the phrase "launch" or "new product." As you can see, it looks like the keyword "launch" identified a product called "Zetia."
```{r illustrate_sectextbykeyword_1}
word_innov <- c("launch", "new product")
innov_text <- which(grepl(paste(word_innov, collapse = "|"), tolower(paragraphs)) == TRUE)

knitr::knit_print(paragraphs[innov_text][11])
```

Then, for these paragraphs containing "launch" or new product, we looked for any non-English words in these text elements. We do see "Zetia" in this list! 
```{r illustrate_sec_nonenglish_1}
knitr::kable(allwordcounts[Company == 310158][1:5])
```


**B) Searching text for protected brands**

Second, for any paragraph, we looked for any word with a "Registered" or "Trademarked" symbol adjacent to it. Below, you can see some registered and trademarked tokens we hope to match to our existing word list. Note that this list can include both English and non-English words. 
```{r illustrate_sec_protected_1}
protectedunique <- unique(allregwords$Words)

knitr::knit_print(head(protectedunique, 3))
knitr::knit_print(protectedunique[protectedunique %in% c("Click®", "Coach®", "Blue®" )])
```



#### Results: Product Capture (Step 3) - Standalone data results from each approach

Wordlist Sets | N (# Word-Mentions)| Unique Words/Tokens
------------- | --------------|------------------
Total Words  | `r nrow(allwords)` |`r length(unique(allwords$Words))`
Non-English words| `r nrow(allwordcounts)` |`r length(unique(allwordcounts$Words))`
Brand-protected words| `r nrow(allregwords)`|`r length(unique(allregwords$Words))`


* Total Words: 
    - Entire sentences transposed by word into vertical columns
    - Each row represents a single word per filing
    - Sequence preserved

```{r illustrate_allfilingwords_2}
allwords[allwords$Company == 1001316,][132:137,] %>% knitr::kable()
```

* Non-English Words: 
    - Words that do not pass English spell check function, grouped by filing
    - Each row represents a grouped count so sequence is lost
    - e.g., each row = COUNT(company x yearly filing x non-English word)
    - ex: CIK Company 1001316's 2013 filing mentions Rituxan 3 times = 1 row


```{r illustrate_nonengwords_2}
allwordcounts[2:6,]  %>% knitr::kable()
```

* Brand-Protected Words: 
    - Words that end R or TM symbols 
    - Each row represents a grouped count so sequence is lost
    - e.g., each row = company  x yearly filing x brand-protected word 
    - ex: CIK Company 1001316's 2013 filing mentions Rituxan® 3 times = 1 row

```{r illustrate_regwords_2}
allregwords %>% count(Company, Year, Words) %>% arrange(Company, Year) %>% filter(Company == 1001316) %>% head(10)  %>% knitr::kable()
```

#### Results: Product Capture (Step 3) - How do Non-English and Protected-Word Results Compare? 

Now that we understand what each of these resulting datasets look like alone, we want to understand what words each approach was *uniquely* able to capture and what words *both* approaches captured. How redundant are these methods? Would a single one of these methods suffice for this work?

The quick answer is no, while both methods appear to capture a subset of words they do uniquely capture distinct words. Running both methods allows us to capture the largest swath of possible products. 

There are two ways to review the results, by company-specific mention of the word or word (regardless of who mentioned). When you review results by which company mentions the word, you can see both methods capture almost 600 words but separately uniquely capture thousands more results. When you review results by word (without referencing which company made the reference), this pattern remains stable. Both methods capture almost 700 words but separately uniquely capture thousands more results. 

```{r makes_mention_venndiagram_2, eval = FALSE}
nonenglish_mentions <- wcbycompdetails %>% transmute(mention_id = paste(Company, Words))
nrow(nonenglish_mentions)
nonenglish_mentions <- unique(nonenglish_mentions$mention_id)
length(nonenglish_mentions)

protected_mentions <- wcbycompdetails_reg %>% transmute(mention_id = paste(Company, str_remove(Words, "®|™")))
nrow(protected_mentions)
protected_mentions <- unique(protected_mentions$mention_id)
length(protected_mentions)

length(nonenglish_mentions)
length(protected_mentions)
length(intersect(nonenglish_mentions, protected_mentions))

# Load library
library(VennDiagram)
library(RColorBrewer)
myCol <- brewer.pal(3, "Pastel2")

# Chart
venn.diagram(
        x = list(nonenglish_mentions, protected_mentions),
        category.names = c("Non-English" , "Brand-Protected"),
        filename = '~/git/business_innovation/src/dnair/images/comp_noneng_brand_mentions.png',
        output=TRUE,
        
        # Output features
        imagetype="png" ,
        height = 480 , 
        width = 480 , 
        resolution = 300,
        compression = "lzw",
        
        # Circles
        lwd = 2,
        lty = 'blank',
        fill = myCol[c(1,3)],

        # Numbers
        cex = .4,
        fontface = "bold",
        fontfamily = "sans",
        
        # Set names
        cat.cex = 0.5,
        cat.fontface = "bold",
        cat.default.pos = "outer",
        cat.pos = c(-27, 180),
        cat.dist = c(0.055, 0.055),
        cat.fontfamily = "sans"

)
```

```{r makes_unqword_venndiagram_2, eval = FALSE}
nonenglish_tokens <- unique(wcbycompdetails$Words)
protected_tokens <- unique(str_remove(wcbycompdetails_reg$Words, "®|™"))

length(nonenglish_tokens)
length(protected_tokens)
length(intersect(nonenglish_tokens, protected_tokens))

# Load library
library(VennDiagram)
library(RColorBrewer)
myCol <- brewer.pal(3, "Pastel2")

# Chart
venn.diagram(
        x = list(nonenglish_tokens, protected_tokens),
        category.names = c("Non-English" , "Brand-Protected"),
        filename = '~/git/business_innovation/src/dnair/images/comp_noneng_brand_tokens.png',
        output=TRUE,
        
        # Output features
        imagetype="png" ,
        height = 480 , 
        width = 480 , 
        resolution = 300,
        compression = "lzw",
        
        # Circles
        lwd = 2,
        lty = 'blank',
        fill = myCol[c(1,3)],

        # Numbers
        cex = .4,
        fontface = "bold",
        fontfamily = "sans",
        
        # Set names
        cat.cex = 0.5,
        cat.fontface = "bold",
        cat.default.pos = "outer",
        cat.pos = c(-27, 180),
        cat.dist = c(0.055, 0.055),
        cat.fontfamily = "sans"

)

```

<div class="col2">
![Overlap of Word Mentions by Company Filing.](~/git/business_innovation/src/dnair/images/comp_noneng_brand_mentions.png)
![Overlap of Unique Words.](~/git/business_innovation/src/dnair/images/comp_noneng_brand_tokens.png)
</div>


```{r collapses_regnoneng_tokens_2}
allregwords <- allregwords %>% count(Company, Year, Words)
allwordcounts$n <- allwordcounts$count
allwordcounts$count <- NULL
wordlist_engreg <- rbind(allwordcounts, allregwords)
#wordlist_engreg %>% arrange(Words)
#nrow(allwordcounts) + nrow(allregwords) 
#nrow(wordlist_engreg)

wordlist_enreg2 <- wordlist_engreg %>% mutate(Protect = str_extract(Words, "®|™"), Token = str_remove(Words, "®|™"))
wordlist_enreg2 <- wordlist_enreg2 %>% group_by(Company, Year, Token, Protect) %>% summarise(n = sum(n))
```

Ultimately, these results give us confidence that both approaches should be applied and the resulting subsets combined. In the newly combined set we now have: `r nrow(wordlist_engreg)` by company mention. You can see the number of times the "Abilify" product was mentioned by various company filings over time, along with where that mention appeared with a brand protection. 

```{r}
wordlist_enreg2 %>% arrange(Token, Year) %>% filter(Token == "Abilify" & Year == 2015) %>% knitr::kable()
```

*Problem:* We would like to believe everything found by these two approaches are product names. However, we observe that the list of words representing candidate products that we have obtained from the filings can also include (i) company names or (ii) sector-specific terms. Therefore,  dictionaries for these groups need to be generated to identify and eliminate them. In other words, we need to refine our results.  

```{r illustrates_refine_problem_2}
wordlist_enreg2[wordlist_enreg2$Token %in% c("Abbvie", "ANDA", "Mallinckrodt", "Pharma", "AstraZeneca", "Bausch"),c("Token", "n")] %>% group_by(Token) %>% summarise(n = sum(n)) %>% knitr::kable()
```


```{r arrangingregovertime_3}
#allregwords <- allregwords %>% count(Company, Year, Words)
#wordlist_enreg2
#wcbycomp_reg <- reshape2::dcast(allregwords, Company + Words ~ Year, value.var = "n", fun.aggregate = sum)
#wcbyword_reg <- reshape2::dcast(allregwords, Words + Company ~ Year, value.var = "n", fun.aggregate = sum)
wcbycomp <- reshape2::dcast(wordlist_enreg2, Company + Token + Protect ~ Year, value.var = "n", fun.aggregate = sum)

wcbycompdetails_reg <- wcbycomp %>%
  left_join(ciknames, by = c("Company" = "CIK")) %>%
  left_join(sic, by = c("Company" = "CIK", "SEC_SIC" = "SIC_SIC")) %>%
  left_join(cik_ticker, by = c("Company" = "CIK"))
```


##### Step 4: Refine Capture Results


**Create two dictionaries to filter against:**

* company dictionary (sub dictionary of company name stop words ex. inc, corp)
* industry stop words ex. biopharma, therapeutics

**Create company name reference set:**

* create columns 
    - concatenated string of all names across CIK codes
    - tokens of company names
    - tokens as lower case
    - stemmed tokens 
* remove tokens less than a character long
* rename columns to indicate source of each name

*Result:* 1 long string of company names combined (Q. First bullet - Is this the best way to do this?)

```{r shaping_data2}
company_reference_names <- companylist %>% distinct() %>% 
  mutate(CompanyString = paste(SIC_Company_Name, SEC_Company_Name, Ticker_Company_Name)) 
knitr::kable(company_reference_names[781:782, c(2, 6, 9, 15)])
```


*Result:* Long company name string is decomposed into its tokens in their:

* original form
* lower case form
* stemmed form

```{r}
comp_tokens <- str_split(company_reference_names$CompanyString, pattern = " |[[:punct:]]")
stopwords <- as.vector(c("inc", "corp", "ltd","plc","llc","hold?ing?s","international","group","acquisition","american","china","usa"))
#stopwords <- paste0( stopwords, collapse = "|")

new_ref_companies <- tibble(company_reference_names$CompanyString, comp_tokens) %>% # head() %>% 
  tidyr::unnest() %>% filter(nchar(comp_tokens) >1) %>% as.data.frame() %>%
  left_join(company_reference_names, by = c("company_reference_names$CompanyString" = "CompanyString")) %>%
  mutate(comp_lowword = str_to_lower(str_squish(comp_tokens))) %>% filter(comp_lowword %!in% stopwords) %>% 
  mutate(comp_low_hun = text_tokens(comp_lowword, stemmer = stem_hunspell)) %>% tidyr::unnest() %>% distinct()

colnames(new_ref_companies) <- c("CompanyString", "Tokens", "CIK", "SIC_Company_Name", "SIC_SIC", "SIC_Industry", "SIC_Location", "SEC_Company_Name", "SEC_SIC", "Ticker_Code", "Ticker_Company_Name", "Ticker_Exchange", "Ticker_SIC_Code", "Ticker_Location", "Ticker_IncLoc", "Ticker_IRS", "token_low", "token_hun")

new_ref_companies %>% filter(str_detect(CompanyString, "Dorato Resources Inc")) %>% select(c(1,2, 17,18)) %>% knitr::kable()
```

*Reduce company token set:* 

* found repeating English tokens 
* remove any (English or non-English) tokens that repeat

*Risk:* (Both ways) You lose any names that are only composed of generic pharma names: ex. Drugs Pharmaceuticals Inc. would be lost
*Risk:* (2nd Method) You may lose companies that are in same corporate family and have same unique-ish token but since it appears for two companies, it's lost: ex. Apogee Technology vs. Apogee Enterprises would both be lost. 

*Result:* Decided repeating English tokens was still too generic and we would want to also remove non-English - below was not used. 
```{r}
multenglishtokens <- new_ref_companies %>% mutate(eng = hunspell::hunspell_check(token_hun)) %>% filter(eng == "TRUE") %>% group_by(token_hun) %>% summarise(n = n()) %>% filter(n > 1)
knitr::kable(multenglishtokens[964:967,])
```

*Result:* Apogee appears as a token that appears in two company names: Apogee Enterprises, Inc and Apotee Technology Inc. Our original set included these two companies with that token. Our new company match set NO LONGER INCLUDES THESE COMPANIES. 

```{r}
multtokens <- new_ref_companies %>% group_by(Tokens) %>% summarise(n = n()) %>% filter(n > 1)
#knitr::kable(multtokens[str_detect(multtokens$Tokens, "Apogee"),])
new_ref_companies2 <- new_ref_companies
new_ref_companies <- new_ref_companies %>% filter(Tokens %!in% multtokens$Tokens)

new_ref_companies2 %>% filter(str_detect(Tokens, "Apogee|APOGEE")) %>% select(1:2) %>% distinct() %>% knitr::kable()
```


*Result:* The companies that remain have uniquely identifiable tokens.  

```{r}
new_ref_companies %>% filter(SIC_SIC == 2834) %>% select(1:2) %>% head(4) %>% knitr::kable()
```

#### Matching Wordlist to Company Names:

FROM: SIC-SEC-Stock Exchange
TO: Non English Words

* generate regex patterns of company name tokens
    - original token
    - lower case token
    - stemmed token
* generate corresponding columns of wordlist
    - lower case word
* detect and extract company name tokens in wordlist

```{r}
patt = paste0("^", new_ref_companies$Tokens, "$", collapse = "|")
pattlow = paste0("^", new_ref_companies$token_low, "$", collapse = "|")
patthun = paste0("^", new_ref_companies$token_hun, "$", collapse = "|")

wcbyword_findcompanies <- wcbycomp %>% mutate(
  word_low = str_to_lower(Token),
  companyTF = str_detect(Token, patt), 
  compmatch = str_extract(Token, patt), 
  complowTF = str_detect(word_low, pattlow),
  complowmatch = str_extract(word_low, patthun))

wcbyword_findcompanies %>% filter(Token %in% c("ENBREL", "Integra")) %>% head() %>% knitr::kable()

```


#### Removing industry terms 

```{r}
# lookforpharmaterms <- wcbycomp %>% count(Token) %>% arrange(desc(n))
pharmstopwords <- c("biopharma", "therapeutics?", "pharmaceuticals?", "international", "sciences?", "medical", "technology",  "pharma?", "bio", "biosciences?", "anda", "fdca", "uspto")

pharmstopwords <- paste0(paste0("\\b", pharmstopwords, "\\b"), collapse = "|")
```

We find 16 terms that altogether represent 246 observations. 


```{r}
wcbyword_findcompanies %>% filter(str_detect(string = word_low, pattern = pharmstopwords) ==  TRUE) %>% select(Token) %>% count(Token) %>% arrange(desc(n)) %>% head(4) %>% knitr::kable()

```



### Results



```{r remove_companies_from_final_wordlist_2 }
companyref <- sic %>% # 58K  
  full_join(ciknames, by = "CIK") %>% # 779      
  full_join(cik_ticker, by = "CIK") %>%  #13K
  full_join(cikcountries, by = c("Ticker_Location" = "Code")) #309 - but doens't affect rows
                                                 
companyref <- companyref %>% 
  mutate(Name = ifelse(!is.na(SEC_CompName), SEC_CompName,
                       ifelse(!is.na(SIC_CompName), SIC_CompName,
                              ifelse(!is.na(Ticker_CompName), Ticker_CompName, paste(SEC_CompName, SIC_CompName, Ticker_CompName))))) %>%
  transmute(Name = Name, CIK = CIK, SIC = SEC_SIC, SIC_Loc = SIC_Location, TickerCode = Ticker_TickerCode, Exchange = Ticker_Exchange, Ticker_Location, Ticker_Inc_Location, Ticker_StateCountry, US)

final_product_list <- wcbyword_findcompanies %>% 
  left_join(companyref, by = c("Company" = "CIK")) %>%
  mutate(Self = ifelse(str_detect(str_to_lower(Name), pattern = complowmatch), 1, 0)) %>%  
  select(-word_low, -companyTF, -compmatch, -complowmatch)

final_product_list$CompanyYN <- final_product_list$complowTF 
final_product_list$complowTF <- NULL

```

```{r}
final_product_list <- final_product_list %>% filter(str_detect(string = str_to_lower(Token), pattern = pharmstopwords) ==  FALSE) 
```




Things we're removing: 

* words that match with companies - 1162 words
* words that look like products but got a self/other result - 29
* words that are pharma industry terms

What's left: 7208 words

```{r look_at_final_sets, eval = FALSE}

final_product_list %>% count(CompanyYN, Self) %>% knitr::kable()

```

Products per company
max - 391
avg - 16
min - 1
sd - 30

```{r}
final_product_list <- final_product_list %>% 
  filter(CompanyYN == FALSE) %>% 
  filter(is.na(Self))

innov_per_company <- final_product_list %>% count(Company, Name)

mean(innov_per_company$n)
max(innov_per_company$n)
min(innov_per_company$n)
sd(innov_per_company$n)
table(innov_per_company$n)
hist((innov_per_company$n[innov_per_company$n < 41]), breaks = c(0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 40), main = "Innovations per Company (1-40)")
```


```{r}
final_product_list %>% count(US)
final_product_list %>% count(SIC_Loc, US)
final_product_list %>% count(SIC_Loc, US, Ticker_StateCountry)

table(is.na(final_product_list$US == "USA"))
```



##### Innovation Trends Per Industry

```{r plotsettings_2n3}
customPlot2 = list(
  theme(#plot.margin = unit(c(1,1,2,2), "cm"),
        axis.text.x  = element_text(vjust=0.5, size=12),
        plot.title=element_text(size=12, vjust=2),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#FFFFFF") , 
        legend.position="right"),  #coord_flip(), 
  guides(fill=guide_legend(title="Key", ncol = 1),
        colour =guide_legend(title="Key", ncol = 1))
)

customPlot3 = list(
  theme(axis.text.x  = element_text(vjust=0.5, size=12), #plot.margin = unit(c(1,1,2,2), "cm"),
        plot.title=element_text(size=12, vjust=2),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), #axis.title.x = element_blank(), axis.title.y = element_blank(),
        panel.background = element_rect(fill = "#FFFFFF") , 
        legend.position="bottom"), 
  coord_flip(), 
  guides(fill=guide_legend(title="Key", ncol = 3),
        colour =guide_legend(title="Key", ncol = 3))
)


```

```{r}
pal_ind <- wes_palette("GrandBudapest1", 3, type = "discrete")
pal_5 <- wes_palette("Moonrise3", 5, type = "discrete")
```


```{r maketime}
time_summ <- final_product_list %>% group_by(SIC) %>% 
  summarise(`2012` = sum(`2012`),
            `2013` = sum(`2013`),
            `2014` = sum(`2014`),
            `2015` = sum(`2015`),
            `2016` = sum(`2016`), 
            `2017` = sum(`2017`)) %>% 
  reshape2::melt(id.vars = "SIC", measure.vars = c("2012", "2013", "2014", "2015", "2016", "2017"), variable.name = "Year" )  %>%
  filter(!is.na(SIC))
  #reshape2::dcast(Year~SIC, fun.aggregate = sum) %>% select(-`NA`)

```

Overall, the pharmaceutical industry appears to show the greatest number of innovations and appear to mention these innovations in their SEC filings more often.

```{r}
ggplot(data = time_summ, aes(x=as.factor(SIC), y=value, fill = as.factor(SIC), group = as.factor(SIC) )) + 
  geom_col() +
  #geom_line(linetype="solid", size=1.5) + 
  #geom_point(size=3, shape=21, fill="white")  +
  customPlot2 +
  scale_color_manual(values = pal_ind)  +
  labs(title = "Number of Unique Products per Industry", x = "Industry", y = "Number of Products")

```


```{r inntimeplot}
ggplot(data = time_summ, aes(x=Year, y=value, color = as.factor(SIC), group = factor(SIC) )) + 
  geom_line(linetype="solid", size=1.5) + 
  geom_point(size=3, shape=21, fill="white")  +
  customPlot2 +
  scale_color_manual(values = pal_ind)  +
  labs(title = "Number of Products Detected per Industry Over Time", x = "Year", y = "Number of Products")
```


```{r}
drugsbytime <- final_product_list %>% 
  count(Token, Protect, `2012`, `2013`, `2014`, `2015`, `2016`, `2017`) %>% 
  mutate(nummentionsallyears = `2012`+`2013`+`2014`+`2015`+`2016`+`2017`) 

drugsbytime$compcount <- drugsbytime$n 
drugsbytime$n <- NULL

drugsbytime_top <- drugsbytime %>% 
  arrange(desc(nummentionsallyears)) %>%
  head(5) %>% 
  mutate(Product = ifelse(!is.na(Protect), paste0(Token, Protect), Token)) %>% 
  select(Product, `2012`, `2013`, `2014`, `2015`, `2016`, `2017`) %>% 
  reshape2::melt(id.vars = "Product", measure.vars = c("2012", "2013", "2014", "2015", "2016", "2017"), variable.name = "Year" )


```


The top 5 products are AGGRASTAT, Fanapt, ONSOLIS, OraQuick, and Vascepa. It is reassuring to note that all of these are in fact products. 

```{r innwtimeplot}
ggplot(data = drugsbytime_top, aes(x=Year, y=value, group = Product, colour = Product)) + 
  geom_line(linetype="solid", size=1.5) + 
  geom_point(size=3, shape=21, fill="white") + 
  scale_color_manual(values = pal_5) +
  customPlot2 +
  labs(title = "Top 5 Products over Time", x = "Year", y = "Number of Mentions")
```

First Mentioned? 


```{r}
firstmention_by_company <- final_product_list %>% 
  transmute(Name = Name, 
        first_mention = ifelse(`2012` > 0, 2012, 
                                ifelse(`2013` > 0, 2013, 
                                        ifelse(`2014` > 0, 2014, 
                                                ifelse(`2015` > 0, 2015, 
                                                        ifelse(`2016` > 0, 2016,
                                                               ifelse(`2017` > 0, 2017, 1900)))))), 
         Product = ifelse(!is.na(Protect), paste0(Token, Protect), Token))

```

```{r}
ggplot(data = firstmention_by_company, aes(x=first_mention)) +  #, y=Product, group = Product, colour = Product)) + 
  geom_histogram(binwidth = 0.5, fill = pal_5[1]) +
  #scale_fill_manual(values = ) +
  customPlot2 +
  labs(title = "First Mentions of Product", x = "Year", y = "Number of Products")

```

436 companies have 7208 drugs/devices


```{r}
final_product_list_better_location <- final_product_list %>% left_join(cikcountries, by = c("SIC_Loc" = "Code")) 

topcompanies <- final_product_list_better_location %>% count(Name) %>% arrange(desc(n)) %>% head(5)
topcompanies <- final_product_list_better_location %>% 
  filter(Name %in% topcompanies$Name) %>% 
  group_by(Name, Ticker_StateCountry.y) %>%
  summarise(`2012` = sum(`2012`), `2013` = sum(`2013`), `2014` = sum(`2014`), `2015` = sum(`2015`), `2016` = sum(`2016`), `2017` = sum(`2017`)) %>%
  reshape2::melt(id.vars = c("Name", "Ticker_StateCountry.y"), measure.vars = c("2012", "2013", "2014", "2015", "2016", "2017"), variable.name = "Year" )
```


```{r}
ggplot(data = topcompanies, aes(x=Year, y=value, group = Name, colour = Name)) + 
  geom_line(linetype="solid", size=1.5) + 
  geom_point(size=3, shape=21, fill="white") + 
  scale_color_manual(values = pal_5) +
  customPlot2 +
  labs(title = "Top 5 Companies over Time", x = "Year", y = "Total Product-Mentions")

```

```{r eval=FALSE}
ggplot(data = topcompanies, aes(x=Year, y=value, group = Ticker_StateCountry.y, colour = Ticker_StateCountry.y)) + 
  geom_line(linetype="solid", size=1.5) + 
  geom_point(size=3, shape=21, fill="white") + 
  scale_color_manual(values = pal_5) +
  customPlot2 +
  labs(title = "Top 5 Companies over Time - Location", x = "Year", y = "Number of Mentions")
  
```
           

```{r}
productspercompany <- final_product_list %>% 
  mutate(Product = ifelse(!is.na(Protect), paste0(Token, Protect), Token)) %>%
  count(Name) 
```



```{r}
ggplot(data = productspercompany, aes(x=n)) +  #, y=Product, group = Product, colour = Product)) + 
  geom_histogram(binwidth = 0.5, fill = pal_5[1]) +
  #scale_fill_manual(values = ) +
  customPlot2 +
  labs(title = "Distribution of # of Unique Products by Company", x = "Number of Products", y = "Number of Companies")
```



```{r maketopcomps}
# counts of unique product mentions by company
customPlot = list(
  theme(#plot.margin = unit(c(1,1,2,2), "cm"),
        axis.text.x  = element_text(vjust=0.5, size=12),
        plot.title=element_text(size=12, vjust=2),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        panel.background = element_rect(fill = "#FFFFFF"), 
        legend.position="bottom"),
  coord_flip()
  , guides(fill=guide_legend(title="Key", nrow = 2))
)
```


