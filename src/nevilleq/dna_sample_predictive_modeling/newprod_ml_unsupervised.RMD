---
title: "Unsupervised ML Methods"
author: "DSPG Business Innovation Team"
date: "7/25/2019"
output:
    github_document: default
    html_document: default
---
  
```{r setup, include=FALSE, message = FALSE, warning = FALSE}
#Libraries
library(jsonlite)
library(tidyverse)
library(janitor)
library(viridis)
library(purrr)
library(data.table)
library(maditr)
library(DataExplorer)
library(Hmisc)
library(DescTools)
library(caret)
library(tm)
library(e1071)
library(patchwork)
library(topicmodels)
library(broom)
library(tidytext)
library(textmineR)

#Setting root directory
knitr::opts_knit$set(echo = TRUE,
                     root.dir = rprojroot::find_rstudio_root_file())

#Controlling figure output in markdown
knitr::opts_chunk$set(
  #  fig.height =   
  fig.width = 6,
  #  fig.asp = .5,
  out.width = "90%",
  #  out.height = 
  cache = TRUE
)
#Set Theme for ggplot2
theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom"))
#Set Scientific notation output for knitr
options(scipen = 999999)
```


```{r dtm_function, echo = FALSE}
#Function to transform data into Document Term Matrix
make_dtm <- function(dna.df, min.threshold) { 

###Transfrom Body Text into a corpus and clean
dna.corpus <- Corpus(VectorSource(dna.df$body)) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c("the", "and", stopwords("english"))) %>%
  tm_map(stripWhitespace)

#Document Term Matrix
dna.dtm.df  <- DocumentTermMatrix(dna.corpus)

#Outcome
new.product <- dna.df$subject_code_ns

#Add Outcome to Document Matrix
dna.doc.dt  <- data.table(doc = dna.dtm.df$i, word = dna.dtm.df$dimnames$Terms[dna.dtm.df$j], count = dna.dtm.df$v) %>%
  merge(data.table(doc = seq_along(new.product), new_product = new.product))


# This gives how many words appear a given number of times in total
wordTable   <- dna.doc.dt[,.(count = sum(count)), by = c('word')] %>% arrange(-count) %>% data.table

# We require a word to appear at least this many times to be included in the analysis
minWordCountThreshold <- min.threshold
survivorWords    <- wordTable[count >= minWordCountThreshold, word]

# Now we look at words broken out by topic
reducedWordCount <- dna.doc.dt[word %in% survivorWords]
reducedWordCount <- data.table(reducedWordCount, filterWordIndex = match(reducedWordCount$word, unique(reducedWordCount$word)))

# Turn into a long format matrix
dna.mat <- matrix(0, nrow = length(dna.corpus), ncol = length(unique(reducedWordCount$word)))
for(i in 1:nrow(reducedWordCount)) dna.mat[reducedWordCount$doc[i], reducedWordCount$filterWordIndex[i]] = reducedWordCount$count[i]

#Names
#rownames(dna.mat) <- paste0('doc', 1:nrow(dna.mat))
colnames(dna.mat) <- unique(reducedWordCount$word)

  return(dna.mat)
}
```



##2. Unsupervised Methods (Bag of Words)

```{r message = FALSE, warning = FALSE}
#Store desired filepath
file.path <- "./data/working/DNA_Aggregated/Machine_learning_sample/NPS_sample_data/"

#Test train sample
train.split <- 0.8
N <- 1000
set.seed(2019)
train.samp <- sample(1:N, train.split * N, replace = FALSE)

#Create tibble of each sample data identified by the variable 'sample', stored in the variable 'data'
sample.df <- list.files(path = file.path) %>%
  enframe() %>%
  rename(file_path = value) %>%
  filter(file_path %>% str_detect(".RDS")) %>%
  mutate(
    sample      = str_split_fixed(file_path, "_", 5)[ ,4],
    file_path   = str_c(file.path, file_path),
    data        = map(.x = file_path, ~read_rds(.x) %>% as_tibble()),
    subject     = map(.x = data, ~.x %>% dplyr::select(subject_code_ns) %>% 
                                  mutate(subject_code_ns = as.factor(subject_code_ns))),
    data        = map2(.x = data, .y = subject, ~bind_cols(as_tibble(make_dtm(.x, 40)), .y) %>%
                                                 dplyr::select(subject_code_ns, everything())),
    train       = map(.x = data, ~.x[train.samp, ]),
    test        = map(.x = data, ~.x[setdiff(1:N, train.samp), ])
  )
```

```{r eval = FALSE}
#Write out json's for Neil 
for(i in 1:nrow(sample.df)) {
  
  write_json(sample.df$data[[i]], sample.df$file_path[i] %>% str_replace(., ".RDS", ".json"))

}

#Remove file path
sample.df <- sample.df %>%
  dplyr::select(-file_path)
```

####a. K-means  

```{r predict_k_means, echo = FALSE}
predict.kmeans <- function(object,
                           newdata,
                           method = c("centers", "classes")) {
  method <- match.arg(method)

  centers <- object$centers
  ss_by_center <- apply(centers, 1, function(x) {
    colSums((t(newdata) - x) ^ 2)
  })
  best_clusters <- apply(ss_by_center, 1, which.min)

  if (method == "centers") {
    centers[best_clusters, ]
  } else {
    best_clusters
  }
}

wss <- function(d) {
  sum(scale(d, scale = FALSE)^2)
}

wrap <- function(hcluster.obj, data) {
#  mod <- cutree(hcluster.obj, k)
  spl <- split(data, hcluster.obj)
  wss <- sum(sapply(spl, wss))
  wss
}
```

```{r k_means}
kmeans.df <- sample.df %>%
  mutate(
    kmean     = map(.x = train, ~kmeans(.x %>% dplyr::select(-subject_code_ns), centers = 2), nstart = 50, iter.max = 100),
    cluster   = map2_dbl(.x = train, 
                     .y = kmean, 
                     ~which.max(
                       c(mean((.y$cluster == 1) == .x$subject_code_ns),
                         mean((.y$cluster == 2) == .x$subject_code_ns))
                               )
                     ),
    k_preds    = map2(.x = kmean, 
                      .y = test, 
                      ~predict.kmeans(.x, 
                                      .y %>% 
                                        dplyr::select(-subject_code_ns) %>% 
                                        as.matrix(), 
                                      method = "classes")),
    k_logical   = map2(.x = k_preds, .y = cluster, ~(.x == .y)),
    k_accuracy  = map2_dbl(.x = k_logical, .y = test, ~mean(.x == .y$subject_code_ns))
  )

#Not splitting well (along NS subject code)
((kmeans.df$kmean[[1]]$cluster == 1) == kmeans.df$test[[1]]$subject_code_ns) %>% mean()
((kmeans.df$kmean[[1]]$cluster == 2) == kmeans.df$test[[1]]$subject_code_ns) %>% mean()

#Accuracy
kmeans.df %>%
  dplyr::select(sample, k_accuracy) %>%
  knitr::kable(digits = 3)


#The model is basically
empirical.proportion <- map_dbl(.x = kmeans.df$train, ~mean(.x$subject_code_ns %>% as.logical()))
preds                <- map2(.x = kmeans.df$test, .y = empirical.proportion, ~rbinom(nrow(.x), 1, .y) == 1)
accuracy             <- map2_dbl(.x = kmeans.df$test, .y = preds, ~mean(.y == .x$subject_code_ns))

#Uninformed Accuracy
tibble(
  sample   = kmeans.df$sample,
  accuracy = accuracy 
) %>% knitr::kable(digits = 3)

#Within Sum of Squares
tibble(
  Sample                        = kmeans.df$sample,
  `Total Within Sum of Squares` = map_dbl(.x = kmeans.df$kmean, ~.x$tot.withinss),
  `Between Sum of Squares`      = map_dbl(.x = kmeans.df$kmean, ~.x$betweenss)
) %>% knitr::kable(digits = 2)
```

The kmeans models do not appear to be seperating particularly well, and are performing poorly. Compared to the most uninformative model, using the estimated proportion to sample from a bernoulli $N$ `TRUE`/`FALSE` with porbability of drawing a true being the estimated proportion of `TRUE` in the training set, on the test set. Our kmeans models are only performing marginally better than the uninformative model; which actually appears to be good when the proportions are small because the models just predict all `FALSE`, which ends up being mostly correct (but it is an entirely inflexible model to variation in the the test data). 

####b. Heirarchical  

```{r, heirarchical}
#Store method vector
methods <- c("average", "complete", "single", "mcquitty", "median", "centroid")

hclust.df <- sample.df %>%
  mutate(
    hclust_average  = map(.x = data, 
                             ~hclust(.x %>% dplyr::select(-subject_code_ns) %>% dist(), method = "average") %>%
                              cutree(., 2)),
    hclust_complete = map(.x = data, 
                             ~hclust(.x %>% dplyr::select(-subject_code_ns) %>% dist(), method = "complete") %>%
                              cutree(., 2)),
    hclust_single   = map(.x = data, 
                             ~hclust(.x %>% dplyr::select(-subject_code_ns) %>% dist(), method = "single") %>%
                              cutree(., 2)),
    hclust_mcquitty = map(.x = data, 
                             ~hclust(.x %>% dplyr::select(-subject_code_ns) %>% dist(), method = "mcquitty") %>%
                              cutree(., 2)),
    hclust_median   = map(.x = data, 
                             ~hclust(.x %>% dplyr::select(-subject_code_ns) %>% dist(), method = "median") %>%
                              cutree(., 2)),
    hclust_centroid = map(.x = data, 
                             ~hclust(.x %>% dplyr::select(-subject_code_ns) %>% dist(), method = "centroid") %>%
                              cutree(., 2))
  )

accuracy <- hclust.df %>%
            dplyr::select(sample, data, hclust_average:hclust_centroid) %>%
            gather(key = method, value = model, hclust_average:hclust_centroid) %>%
            mutate(
              `Within Sum of Squares` = map2_dbl(.x = model, 
                                                 .y = data,
                                                 ~wrap(.x, .y %>% dplyr::select(-subject_code_ns))),
              cluster       = map2_dbl(.x = data, 
                                       .y = model, 
                                        ~which.max(
                                        c(mean((.y == 1) == .x$subject_code_ns),
                                        mean((.y == 2) == .x$subject_code_ns))
                                     )
                                  ),
              logical       = map2(.x = model, .y = cluster, ~(.x == .y)),
              Accuracy  = map2_dbl(.x = logical, .y = data, ~mean(.x == .y$subject_code_ns))
            ) %>%
  dplyr::select(-c(data, cluster, logical, model))

#Visualize
accuracy %>%
  knitr::kable(digits = 3)

#Plot
accuracy %>%
  mutate(sample = as.factor(sample) %>% fct_relevel("prop", "ten", "twenty", "half")) %>%
  gather(key = metric, value = value, `Within Sum of Squares`:Accuracy) %>%
  filter(metric == "Within Sum of Squares") %>% 
  ggplot(aes(x = sample, y = value, colour = sample, fill = sample)) +
  labs(
    x = "Sample",
    y = "Within Sum of Squares",
    title = "Hierarchical Clustering Within Sum of Squares"
  ) +
  geom_col() +
  coord_flip() +
  facet_wrap(~method, ncol = 3, nrow = 2) +
  scale_colour_viridis_d() +
  scale_fill_viridis_d()

#Plot
accuracy %>%
  mutate(sample = as.factor(sample) %>% fct_relevel("prop", "ten", "twenty", "half")) %>%
  gather(key = metric, value = value, `Within Sum of Squares`:Accuracy) %>%
  filter(metric == "Accuracy") %>% 
  ggplot(aes(x = sample, y = value, colour = sample, fill = sample)) +
  labs(
    x = "Sample",
    y = "Accuracy",
    title = "Hierarchical Clustering 'Accuracy'"
  ) +
  geom_col() +
  coord_flip() +
  facet_wrap(~method, ncol = 3, nrow = 2) +
  scale_colour_viridis_d() +
  scale_fill_viridis_d()
```

Same problem as Kmeans.  

####c. Bayesian

```{r bclust, eval = FALSE}
#Store method vector
bclust.df <- sample.df %>%
  mutate(
    bclust    = map(.x = data, 
               ~bclust(.x %>% dplyr::select(-subject_code_ns) %>% dist())),
    b_h_clust = map(bclust, ~.x$hclust %>% cutree(., 2)),
    wss       = map2_dbl(.x = data, .y = b_h_clust, ~wrap(.y, .x %>% dplyr::select(-subject_code_ns))),
    cluster   = map2_dbl(.x = data, 
                         .y = b_h_clust, 
                         ~which.max(
                              c(mean((.y == 1) == .x$subject_code_ns),
                              mean((.y == 2) == .x$subject_code_ns))
                              )
                         ),
    logical   = map(.x = b_h_clust, .y = cluster, ~(.x == .y)),
    Accuracy  = map2_dbl(.x = logical, .y = data, ~mean(.x == .y$subject_code_ns)) 

  ) %>%
  dplyr::select(c(sample, `Within Sum of Squares`, Accuracy)) %>%
  rename(`Within Sum of Squares` = wss)

#Visualize
bclust.df %>%
  knitr::kable(digits = 3)

#Plot
bclust.df %>%
  mutate(sample = as.factor(sample) %>% fct_relevel("prop", "ten", "twenty", "half")) %>%
  gather(key = metric, value = value, `Within Sum of Squares`:Accuracy) %>%
  ggplot(aes(x = sample, y = value, colour = sample, fill = sample)) +
  labs(
    x = "Sample",
    title = "Hierarchical Clustering Within Sum of Squares"
  ) +
  geom_col() +
  coord_flip() +
  facet_wrap(~metric, ncol = 2, nrow = 1, scales = "free_x") +
  scale_colour_viridis_d() +
  scale_fill_viridis_d()

```

####d. Latent Dirichlet Allocation


#####i. Exploration  

```{r warning = FALSE}
remove_zeroes <- function(x) {
  ifelse(which(x[ ,-1] %>% apply(., 1, function(y) {all(y == 0)})) %>% is_empty(),
       return(x),
       return(x %>%
              slice(-(which(x[, -1] %>% apply(., 1, function(y) {all(y == 0)})))))
       )
}

lda.df <- sample.df %>%
  mutate(
    data      = map(.x = data, ~remove_zeroes(.x)),
    lda       = map(.x = data, 
               ~LDA(.x[ ,-1], k = 2, control = list(seed = 2019))),
    lda_probs = map(.x = lda, ~tidytext::tidy(.x, matrix = "gamma") %>%
                               spread(topic, gamma)),
    lda_preds = map(.x = lda_probs, ~ifelse(.x$`2` > 0.5, 2, 1)),
    cluster   = map2_dbl(.x = data, 
                         .y = lda_preds, 
                         ~which.max(
                              c(mean((.y == 1) == .x$subject_code_ns),
                              mean((.y == 2) == .x$subject_code_ns))
                              )
                         ),
    logical   = map(.x = lda_preds, .y = cluster, ~(.x == .y)),
    Accuracy  = map2_dbl(.x = logical, .y = data, ~mean(.x == .y$subject_code_ns)) 
  )


lda.df %>%
  select(sample, Accuracy) %>%
  mutate(sample = as.factor(sample) %>%
                  forcats::fct_relevel("prop", "ten", "twenty", "half")) %>%
  knitr::kable()
```



#####ii. Prediction  

Alternate method to assess the LDA topic modeling as a predictive model.  


```{r diagnostic, echo = FALSE}
#ROC
roc.log <- function(test, preds){
  probs <- seq(0,1,by=0.01)
  roc.log <- matrix(0,nrow=length(probs),ncol=2)
  result <- test$subject_code_ns %>% as.logical()
  i <- 1
  for(p in probs){
    pred <- preds > p
    ##False positive rate
    false.pos <- sum(!result & pred)/sum(!result)
    ##True postive rate
    true.pos <- sum(result & pred)/sum(result)
    roc.log[i, ] <- c(false.pos, true.pos)
    i <- i + 1  
  }
  return(roc.log)
}

#Plot ROC
plot.roc <- function(roc.log, charstring){
  data.frame(FP = rev(roc.log[ ,1]), TP=rev(roc.log[ ,2])) %>% 
    ggplot()+
    geom_point(aes(FP, TP), color="red", size=.5) +
    geom_line(aes(FP, TP), color="blue") +
    geom_abline(slope = 1, linetype = 2, colour = "lightgrey") +
    labs(
      x = "False Positive",
      y = "True Positive",
      title = charstring
    )
}

#Calculate AUC
auc <- function(roc){
  len <- nrow(roc)
  ##The "delta X" values
  delta <- roc[-1,1]-roc[-len,1]
  ##The "heights" the rectangle (drop the first or last).
  hgt <- roc[-1,2]
  ##The Riemann Sum
  sum(-delta*hgt)
}

#Calculate Cost
cost.func <- function(t, p){
    if(t & !p)
        return(2) #False positive weight
    if(!t & p)
        return(5) #False Negative
  if(t & p)
    return(-1) #True Positive prediction
    return(0)  #True Negative
}

#Cost.min 
cost.df <- function(test, preds) {
  probs  <- seq(0,1,by=0.01)
  result <- test$subject_code_ns %>% as.logical()
  cost   <- vector(mode = "numeric", length = length(probs))
  i <- 1
  for(p in probs){
    pred <- preds > p
  #  frame <- with(test, cbind(as.logical(subject_code_ns), pred))
    cost[i] <- map2_dbl(.x = result, .y = pred, ~cost.func(.x, .y)) %>% sum()
    i <- i+1
  }
  return(tibble(probability = probs, cost = cost))
}

#Plot Cost.min
plot.cost <- function(cost.df, charstring) {
    min.cost <- which.min(cost.df$cost)
    
    cost.df %>% 
    ggplot(aes(probability, cost))+
    geom_point(color="red", size=.5) +
    geom_step(color="blue") +
    labs(
      x = "Probability Threshold",
      y = "Cost",
      title = sprintf("%s Cost: p = 0.%i, cost = %i", charstring, as.integer(cost.df$probability[min.cost] * 100), cost.df$cost[min.cost])
    )
}

```



```{r warning = FALSE, message = FALSE, error = FALSE, eval = FALSE}
#Store desired filepath
file.path <- "./data/working/DNA_Aggregated/Machine_learning_sample/NPS_sample_data/"

#Test train sample
train.split <- 0.8
N <- 1000
set.seed(2019)
train.samp <- sample(1:N, train.split * N, replace = FALSE)

#Create tibble of each sample data identified by the variable 'sample', stored in the variable 'data'
sample.df <- list.files(path = file.path) %>%
  enframe() %>%
  rename(file_path = value) %>%
  filter(file_path %>% str_detect(".RDS")) %>%
  mutate(
    sample      = str_split_fixed(file_path, "_", 5)[ ,4],
    file_path   = str_c(file.path, file_path),
    data        = map(.x = file_path, ~read_rds(.x) %>% as_tibble()),
    subject     = map(.x = data, ~.x %>% dplyr::select(subject_code_ns) %>% 
                                  mutate(subject_code_ns = as.factor(subject_code_ns))),
    data        = map2(.x = data, .y = subject, ~bind_cols(make_dtm(.x, 40), .y) %>%
                                                 dplyr::select(subject_code_ns, everything())),
    train       = map(.x = data, ~.x[train.samp, ]),
    test        = map(.x = data, ~.x[setdiff(1:N, train.samp), ])
  )

```

```{r eval = FALSE}
lda.df <- sample.df %>%
  mutate(
    lda       = map(.x = train, 
               ~FitLdaModel(.x[ ,-1], k = 2, method = "gibbs", iterations = 500, burnin = 100))),
    lda_probs = map(.x = lda, ~tidytext::tidy(.x, matrix = "gamma") %>%
                               spread(topic, gamma)),
    lda_preds = map(.x = lda_probs, ~ifelse(.x$`2` > 0.5, 2, 1)),
    cluster   = map2_dbl(.x = data, 
                         .y = lda_preds, 
                         ~which.max(
                              c(mean((.y == 1) == .x$subject_code_ns),
                              mean((.y == 2) == .x$subject_code_ns))
                              )
                         ),
    logical   = map(.x = lda_preds, .y = cluster, ~(.x == .y)),
    Accuracy  = map2_dbl(.x = logical, .y = data, ~mean(.x == .y$subject_code_ns)) 
  )



```
