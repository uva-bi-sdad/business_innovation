---
title: "Final New Product ML"
author: "Quinton Neville"
date: "7/31/2019"
output:
    github_document: default
    html_document: default
---
  
```{r setup, include=FALSE, message = FALSE, warning = FALSE}
#Libraries
library(jsonlite)
library(tidyverse)
library(janitor)
library(viridis)
library(purrr)
library(data.table)
library(maditr)
library(DataExplorer)
library(Hmisc)
library(DescTools)
library(caret)
library(tm)
library(e1071)
library(patchwork)
library(topicmodels)
library(broom)
library(tidytext)
library(textmineR)
library(doParallel)
library(iterators)
library(foreach)

#Setting root directory
knitr::opts_knit$set(echo = TRUE,
                     root.dir = rprojroot::find_rstudio_root_file())

#Controlling figure output in markdown
knitr::opts_chunk$set(
  #  fig.height =   
  fig.width = 6,
  #  fig.asp = .5,
  out.width = "90%",
  #  out.height = 
  cache = TRUE
)
#Set Theme for ggplot2
theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom"))
#Set Scientific notation output for knitr
options(scipen = 999999)
```


```{r dtm_func, echo = FALSE}
#Function to transform data into Document Term Matrix
make_dtm <- function(dna.df, min.threshold) { 

###Transfrom Body Text into a corpus and clean
dna.corpus <- Corpus(VectorSource(dna.df$body)) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c("the", "and", stopwords("english"))) %>%
  tm_map(stripWhitespace)

#Document Term Matrix
dna.dtm.df  <- DocumentTermMatrix(dna.corpus)

#Outcome
new.product <- dna.df$subject_code_ns

#Add Outcome to Document Matrix
dna.doc.dt  <- data.table(doc = dna.dtm.df$i, word = dna.dtm.df$dimnames$Terms[dna.dtm.df$j], count = dna.dtm.df$v) %>%
  merge(data.table(doc = seq_along(new.product), new_product = new.product))


# This gives how many words appear a given number of times in total
wordTable   <- dna.doc.dt[,.(count = sum(count)), by = c('word')] %>% arrange(-count) %>% data.table

# We require a word to appear at least this many times to be included in the analysis
minWordCountThreshold <- min.threshold
survivorWords    <- wordTable[count >= minWordCountThreshold, word]

# Now we look at words broken out by topic
reducedWordCount <- dna.doc.dt[word %in% survivorWords]
reducedWordCount <- data.table(reducedWordCount, filterWordIndex = match(reducedWordCount$word, unique(reducedWordCount$word)))

# Turn into a long format matrix
dna.mat <- matrix(0, nrow = length(dna.corpus), ncol = length(unique(reducedWordCount$word)))
for(i in 1:nrow(reducedWordCount)) dna.mat[reducedWordCount$doc[i], reducedWordCount$filterWordIndex[i]] = reducedWordCount$count[i]

#Names
#rownames(dna.mat) <- paste0('doc', 1:nrow(dna.mat))
colnames(dna.mat) <- unique(reducedWordCount$word)

  return(dna.mat)
}
```


```{r diagnostic_funcs, echo = FALSE}
#ROC
roc.log <- function(test, preds){
  probs   <- seq(0, 1, by = 0.01)
  roc.log <- matrix(0, nrow = length(probs), ncol = 2)
  result  <- test$subject_code_ns %>% as.logical()
  i <- 1
  for(p in probs){
    pred         <- preds > p
    ##False positive rate
    false.pos    <- sum(!result & pred)/sum(!result)
    ##True postive rate
    true.pos     <- sum(result & pred)/sum(result)
    roc.log[i, ] <- c(false.pos, true.pos)
    i <- i + 1  
  }
  roc.log <- apply(roc.log, 2, rev)
  colnames(roc.log) <- c("FP", "TP")
  return(as_tibble(roc.log))
}

#Plot ROC
plot.roc <- function(roc.log, charstring){
  data.frame(FP = rev(roc.log[ ,1]), TP = rev(roc.log[ ,2])) %>% 
    ggplot()+
    geom_point(aes(FP, TP), color = "red", size = .5) +
    geom_line(aes(FP, TP), color = "blue") +
    geom_abline(slope = 1, linetype = 2, colour = "lightgrey") +
    labs(
      x = "False Positive",
      y = "True Positive",
      title = charstring
    )
}

#Calculate AUC
auc_func <- function(roc){
  len <- nrow(roc)
  ##The "delta X" values
  delta <- roc[-1,1]-roc[-len,1]
  ##The "heights" the rectangle (drop the first or last).
  hgt <- roc[-1,2]
  ##The Riemann Sum
  sum(delta*hgt)
}

#Calculate Cost
cost.func <- function(t, p){
    if(t & !p)
        return(2) #False positive weight
    if(!t & p)
        return(5) #False Negative
  if(t & p)
    return(-1) #True Positive prediction
    return(0)  #True Negative
}

#Cost.min 
cost.df <- function(test, preds) {
  probs  <- seq(0, 1, by=0.01)
  result <- test$subject_code_ns %>% as.logical()
  cost   <- vector(mode = "numeric", length = length(probs))
  i <- 1
  for(p in probs){
    pred <- preds > p
  #  frame <- with(test, cbind(as.logical(subject_code_ns), pred))
    cost[i] <- map2_dbl(.x = result, .y = pred, ~cost.func(.x, .y)) %>% sum()
    i <- i+1
  }
  return(tibble(probability = probs, cost = cost))
}

```

```{r cluster_func, echo = FALSE}
predict.kmeans <- function(object,
                           newdata,
                           method = c("centers", "classes")) {
  method <- match.arg(method)

  centers <- object$centers
  ss_by_center <- apply(centers, 1, function(x) {
    colSums((t(newdata) - x) ^ 2)
  })
  best_clusters <- apply(ss_by_center, 1, which.min)

  if (method == "centers") {
    centers[best_clusters, ]
  } else {
    best_clusters
  }
}

lda_predict <- function(lda, test.data, train.data) {
    #find which cluster has more postive response
    cluster <- ifelse(lda$theta[,2] > 0.5, 2, 1)
    cluster <- which.max(c(mean((cluster == 1) == train.data$innovation),
                          mean((cluster == 2) == train.data$innovation)))
    #Return Predicted Probability of positive response
    list(lda = predict(lda, test.data, k = 2, method = "gibbs", iterations = iter, burnin = burn)[ ,cluster])
      }
wss <- function(d) {
  sum(scale(d, scale = FALSE)^2)
}

wrap <- function(hcluster.obj, data) {
#  mod <- cutree(hcluster.obj, k)
  spl <- split(data, hcluster.obj)
  wss <- sum(sapply(spl, wss))
  wss
}
```

#1. Read, Tidy, and Split the Data

```{r warning = FALSE, message = FALSE}
#Test train sample
train.split <- 0.8
N <- 1000
set.seed(2019)
train.samp <- sample(1:N, train.split * N, replace = FALSE)

#Data read
dna.df <- readRDS("./data/working/DNA_Aggregated/Machine_learning_sample/NPS_sample_data/2019_7_23_half_sample.RDS")

#Create tibble of each sample data identified by the variable 'sample', stored in the variable 'data'
sample.df <- bind_cols(dna.df %>% 
                         make_dtm(., 40) %>%
                         as_tibble(),
                       dna.df %>% dplyr::select(subject_code_ns) %>%
                         mutate(subject_code_ns = as.factor(subject_code_ns))) %>%
             dplyr::select(subject_code_ns, everything())

#Test train split
train.df <- sample.df %>% slice(train.samp)
test.df  <- sample.df %>% slice(-train.samp)

#LDA Sample.df & test/train (different data format)
lda.sample <- CreateDtm(doc_vec = dna.df$body,
                                  doc_names = dna.df$an,
                                  ngram_window = c(1, 1),
                                  stopword_vec = c(stopwords::stopwords("en"),
                                                   stopwords::stopwords(source = "smart")),
                                  verbose = FALSE,
                                  cpus = 4)
lda.sample <- lda.sample[ ,colSums(lda.sample) >= 40]
train.lda  <- lda.sample[ train.samp, ]
test.lda   <- lda.sample[-train.samp, ]
```

#2. Tune and Fit Models

```{r}
#LDA Gibbs Parameters (iterations should be 1000, testing is needed for proper burn in paramter)
iter <- 300
burn <- 200

#Fit Models
a <- Sys.time()
mod.list <- list(
  glm = train(subject_code_ns ~ ., data = train.df, method = "glmnet", family = "binomial",
             trControl = trainControl(method = "cv", number = 5)),
  rf  = train(subject_code_ns ~ ., data = train.df, method = "rf", family = "bernoulli",
             trControl = trainControl(method = "cv", number = 5)),
  gbm = train(subject_code_ns ~ ., data = train.df, distribution = "bernoulli", method = "gbm",
             trControl = trainControl(method = "cv", number = 5)),
  #svm = ,
  #nn  = ,
  lda   = FitLdaModel(train.lda, k = 2, method = "gibbs",
                                 iterations = iter, burnin = burn,
                                 optimize_alpha = TRUE)
  )

b <- Sys.time()
(b - a)

nnet.df <- read_csv("./data/working/DNA_Aggregated/Machine_learning_sample/innovation_sample_data/ROCData-NewSubjectNeuralNet.csv") %>%
  dplyr::select(-X1) %>%
  mutate(
    actual = ifelse(actual %in% 1, TRUE, FALSE) %>% as.factor()
  ) %>%
  rename(subject_code_ns = actual)

pred.df <- tibble(
  model = c(names(mod.list), "BERT"),
 # fit   = mod.list,
  type  = c(rep("Supervised", 3), "Unsupervised", "Supervised"),
  preds = c(map(mod.list[-4], ~predict(.x, newdata = test.df, type = "prob")[ ,2]), lda_predict(mod.list[[4]], test.lda, train.df), list(nnet.df$pred))
)
```


#3. Diagnostics 

####a. Reciever Operator Characteristic Curves (AUC)
```{r message = FALSE}
#ROC
pred.df <- bind_rows(pred.df %>%
  filter(model != "BERT") %>%
  mutate(
    roc = map(preds, ~roc.log(test.df, .x)),
    auc = map_dbl(roc, auc_func)
  ), 
  pred.df %>%
  filter(model %in% "BERT") %>%
  mutate(
    roc = map(preds, ~roc.log(nnet.df, .x)),
    auc = map_dbl(roc, auc_func)
  ))

roc.gg <- pred.df %>%
  mutate(
#    roc = map(preds, ~roc.log(test.df, .x)),
#    auc = map_dbl(roc, auc_func),
    model = as.factor(model) %>% fct_recode("LDA" = "lda", "GLM" = "glm", "Random Forest" = "rf", "Gradient Boosting" = "gbm") %>% fct_reorder(., auc, .desc = TRUE)
  ) %>%
  unnest(roc) %>%
  ggplot(aes(x = FP, y = TP, colour = model)) +
  geom_abline(slope = 1, linetype = 2, colour = "lightgrey") +
  geom_step(size = 1.2, alpha = 0.4) +
  geom_smooth(size = 1.2, alpha = 0.6, se = F, span = 1) +
#  geom_point(size = 1.2, alpha = 0.4, position = "jitter") +
  labs(
    x = "False Positive Rate",
    y = "True Positive Rate",
    title = "ROC Curves by Model"
  ) + 
  scale_colour_viridis_d("Model") +
  theme(legend.position = "bottom")

roc.gg
```

```{r echo = FALSE, include = FALSE}
ggsave("./src/nevilleq/dna_ml_final/nps_ml_figures/19_7_31_nps_roc_2.jpg", roc.gg)
```

####b. Cost 

```{r message = FALSE}
#Cost
pred.df <- bind_rows(
  pred.df %>%
    filter(model != "BERT") %>%
    mutate(
    cost = map(preds, ~cost.df(test.df, .x))
    ),
  pred.df %>%
    filter(model %in% "BERT") %>%
    mutate(
    cost = map(preds, ~cost.df(nnet.df, .x))
    )
  )

cost.gg <- pred.df %>%
  mutate(
    model = as.factor(model)
  ) %>%
  unnest(cost) %>%
  mutate(model = fct_recode(model, "LDA" = "lda", "GLM" = "glm", "Random Forest" = "rf", "Gradient Boosting" = "gbm", "BERT" = "BERT") %>% fct_reorder(cost, min)) %>%
  ggplot(aes(x = probability, y = cost, colour = model)) +
  geom_step(size = 1.2, alpha = 0.4) +
#  geom_point(size = 1.2, alpha = 0.4, position = "jitter") +
  geom_smooth(size = 1.2, alpha = 0.6, se = F, span = 0.6) +
  labs(
    x = "Probability Threshold",
    y = "Cost",
    title = "False Negative Adverse Cost by Model"
  ) +
  scale_colour_viridis_d("Model")

cost.gg
```


```{r echo = FALSE, include = FALSE}
ggsave("./src/nevilleq/dna_ml_final/nps_ml_figures/19_7_31_nps_cost_2.jpg", cost.gg)
```


#4. Final Cost Optimized Result

Accuracy, precision, recall
```{r}
#test.df <- test.df %>%
#  mutate(
#    subject_code_ns = as.logical(subject_code_ns)
#  )

accuracy <- function(preds, threshold, test.df) {
  
  mean((preds > threshold) == test.df$subject_code_ns)

  }

precision <- function(preds, threshold, test.df) {
  
  preds     <- preds > threshold
  true.pos  <- sum(preds & as.logical(test.df$subject_code_ns))
  false.pos <- sum(!as.logical(test.df$subject_code_ns) & preds)
  return(true.pos / (true.pos + false.pos))

}

recall <- function(preds, threshold, test.df) {
  
  preds     <- preds > threshold
  true.pos  <- sum(preds & as.logical(test.df$subject_code_ns))
  false.neg <- sum(!preds & as.logical(test.df$subject_code_ns))
  return(true.pos / (true.pos + false.neg))
  
}

opt.result <- bind_rows(
  pred.df %>%
   filter(model != "BERT") %>%
   mutate(
     min_cost_threshold = map_dbl(.x = cost, ~.x$probability[which.min(.x$cost)]),
     accuracy           = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~accuracy(.x, .y, test.df)),
     precision          = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~precision(.x, .y, test.df)),
     recall             = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~recall(.x, .y, test.df))
   ),
  pred.df %>%
   filter(model %in% "BERT") %>%
   mutate(
     min_cost_threshold = map_dbl(.x = cost, ~.x$probability[which.min(.x$cost)]),
     accuracy           = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~accuracy(.x, .y, nnet.df)),
     precision          = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~precision(.x, .y, nnet.df)),
     recall             = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~recall(.x, .y, nnet.df))
   )
  
 )

opt.result %>%
  dplyr::select(model, accuracy:recall) %>%
  mutate(
    model = fct_recode(model, "LDA" = "lda", "GLM" = "glm", "Random Forest" = "rf", "Gradient Boosting" = "gbm")
  ) %>%
  rename(Model = model, Accuracy = accuracy, Precision = precision, Recall = recall) %>%
  knitr::kable(digits = 3)
```

#5. Internal Validation


####a. Set up Parallelizing
```{r}
#Repeated Train/Test split
n.boot       <- 100
boot.resamps <- list()

#Create n.boot bootstrapped sample indices ("training set")
for(i in 1:n.boot) { 
  boot.resamps[[i]] <- sample(1:N, N, replace = TRUE)
}
#Find the out of bag observations (not in the boot samp, these are "test")
out.of.bag <- map(.x = 1:n.boot, ~setdiff(1:N, boot.resamps[[.x]]))

boot_diagnostics <- function(i) {
  
  mod.list <- list(
  glm = train(subject_code_ns ~ ., data = sample.df[boot.resamps[[i]], ], method = "glmnet", family = "binomial",
             trControl = trainControl(method = "cv", number = 5)),
  rf  = train(subject_code_ns ~ ., data = sample.df[boot.resamps[[i]], ], method = "rf", family = "bernoulli",
             trControl = trainControl(method = "cv", number = 5)),
  gbm = train(subject_code_ns ~ ., data = sample.df[boot.resamps[[i]], ], distribution = "bernoulli", method = "gbm",
             trControl = trainControl(method = "cv", number = 5))
  #svm = ,
  #nn  = ,
#  lda   = FitLdaModel(lda.sample[boot.resamps[[i]], ], k = 2, method = "gibbs",
#                                 iterations = iter, burnin = burn,
#                                 optimize_alpha = TRUE)
  )
  
  pred.df <- tibble(
  model = names(mod.list),
  fit   = mod.list,
  type  = rep("Supervised", 3),# "Unsupervised"),
  preds = map(mod.list, ~predict(.x, newdata = sample.df[out.of.bag[[i]], ], type = "prob")[ ,2])#, lda_predict(mod.list[[4]], lda.sample[out.of.bag[[i]], ], sample.df[boot.resamps[[i]], ]))
  )
  
  opt.result <- pred.df %>%
  mutate(
  #  subject_code_ns    = map(sample.df[out.of.bag[[i]], ]$subject_code_ns, ~as.logical),
    roc                = map(preds, ~roc.log(sample.df[out.of.bag[[i]], ], .x)),
    auc                = map_dbl(roc, auc_func),
    cost               = map(preds, ~cost.df(sample.df[out.of.bag[[i]], ], .x)),
    min_cost_threshold = map_dbl(.x = cost, ~.x$probability[which.min(.x$cost)]),
    accuracy           = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~accuracy(.x, .y, sample.df[out.of.bag[[i]], ])),
    precision          = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~precision(.x, .y, sample.df[out.of.bag[[i]], ])),
    recall             = map2_dbl(.x = preds, .y = min_cost_threshold,
                                  ~recall(.x, .y, sample.df[out.of.bag[[i]], ])),
    iteration          = rep(i, length(recall))
  ) 

return(opt.result %>% dplyr::select(iteration, min_cost_threshold, model, accuracy:recall, auc))
  
}

#Test
a <- Sys.time()
#boot_diagnostics(1)
b <- Sys.time()

b - a
```

####b. Parallelize Out of Bag Results
```{r eval = FALSE}
n.boot  <- 100
nCores  <- 5
taskFun <- boot_diagnostics

registerDoParallel(nCores)

#combine <- function(x, ...) {  
#  mapply(rbind, x, ..., SIMPLIFY = FALSE)
#}
a <- Sys.time()
#100 X 10 = 1000 iterations
result <- foreach(i = 1:100, .combine = rbind,
                         .packages = c("tidyverse", "caret")) %dopar% {
  outSub <- taskFun(i)
  outSub
} 
b <- Sys.time()
b - a

write_rds(result, "./src/nevilleq/dna_ml_final/nps_result/19_7_31_nps_boot_result.RDS")

```

####b. Visualize Results
```{r eval = FALSE}
violin.mse <- read_rds("./src/nevilleq/dna_ml_final/nps_result/19_7_31_nps_boot_result.RDS")
  gather(key = model, value = `Out of Bag Error`) %>% 
  mutate(model = as.factor(model), 
         model = fct_recode(model, "LDA" = "lda", "GLM" = "glm", "Random Forest" = "rf", "Gradient Boosting" = "gbm"),
         model = fct_reorder(model, `Out of Bag Error`, .desc = FALSE, .fun = median)) %>% 
  ggplot(aes(x = model, y = `Out of Bag Error`)) + 
  geom_violin(aes(fill = model), trim = FALSE, alpha = 0.3) + 
  geom_boxplot(width = 0.25) +
  labs(
    y = "Out of Bag Bootsrapped Error",
    x = "Model",
    title = sprintf("Bootstrap Validation: %i Iterations", n.boot)
  ) +
  viridis::scale_fill_viridis(
    option = "magma",
    name = "",
    begin = 1,
    end = 0,
    discrete = TRUE) 

```
